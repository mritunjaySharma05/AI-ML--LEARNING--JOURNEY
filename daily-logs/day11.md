# Day 11 â€“ K-Nearest Neighbors (KNN) Classifier

**Date:** May 26, 2025  

---

## ğŸ” What I Learned Today

### ğŸ”¹ K-Nearest Neighbors (KNN)
- A **lazy learning algorithm**: no training happens during `.fit()`.
- Classifies based on the **majority class** among its **K nearest neighbors**.
- Works well for **small and clean datasets**.

---

## ğŸ› ï¸ Steps Practiced

| Step                 | Description                                                             |
|----------------------|-------------------------------------------------------------------------|
| ğŸ“¥ Libraries Used     | `pandas`, `sklearn`, `KNeighborsClassifier`                             |
| ğŸ“Š Dataset           | Included `Marks` and `Result` columns                                   |
| ğŸ”„ Preprocessing     | Split into features `X` and labels `y`                                   |
| ğŸ¤– Model Training    | Used `KNeighborsClassifier(n_neighbors=3)`                              |
| ğŸ“ˆ Evaluation        | Used `accuracy_score` for measuring performance                         |
| ğŸ–¼ï¸ Visualization     | Plotted the decision boundary to understand how KNN classifies data     |

---

## ğŸ“Š Output & Accuracy

- The model drew a **clear decision boundary** between Pass/Fail.
- **Accuracy** was high due to:
  - Clean, separable data
  - Proper K-value selection

---

## ğŸ§  Notebook

Check full implementation here: [`day11_knn.ipynb`](day11notes.ipynb)
