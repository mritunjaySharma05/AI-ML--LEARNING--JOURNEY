# Day 20 â€“ Random Forest Revisited ðŸŒ²ðŸŒ²

**Date:** June 8, 2025

---

## ðŸ§  Key Concepts Recap

| Concept                  | Explanation                            |
|--------------------------|----------------------------------------|
| Random Forest            | Collection of Decision Trees           |
| `RandomForestClassifier` | Model from sklearn                     |
| `n_estimators=100`       | Number of trees                        |
| Pros                     | Higher accuracy, less overfitting      |
| Cons                     | Harder to interpret than a single tree |

---

## ðŸ”§ What You Practiced

- Loaded clean dataset with binary labels
- Trained a `RandomForestClassifier(n_estimators=100)`
- Evaluated using `accuracy_score` and visual plots
- Compared performance to a single Decision Tree

---

## ðŸ“ˆ Insights

- Random Forest outperformed basic tree due to ensemble learning  
- Robust against overfitting on small datasets  
- Interpretation can be harder, but worth it!

---

ðŸ““ Practice Notebook: [`day20.ipynb`](daily_logs/day20.ipynb)
